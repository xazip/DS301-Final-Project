{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine_Learning_Project.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHmAUkgZ3DtX",
        "colab_type": "code",
        "outputId": "8951da3c-5c9d-41fe-ee31-f36aa6716189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import sklearn\n",
        "assert sklearn.__version__>= '0.20'\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__>= '2.0'\n",
        "\n",
        "from tensorflow import keras "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJmQc03dpQls",
        "colab_type": "code",
        "outputId": "cb26434b-75e7-45a0-b370-6bdd81d21a37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "#Check to make sure versions are correct\n",
        "tf.__version__, keras.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('2.0.0', '2.2.4-tf')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I_2na8npTRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import standard libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJrpBILOjeOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()  # for plot styling"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HjlqzM8pWfk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ignore annoying warnings\n",
        "import warnings \n",
        "warnings.filterwarnings(action = 'ignore', message = '^internal gelsd')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRovejB7rUdd",
        "colab_type": "code",
        "outputId": "93180045-6681-4719-f1d4-0398964d8d2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buvQ9L0mn2vA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CU4lXEAjUuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = r\"/content/drive/My Drive/PeMS_Data_Folder/2008\"\n",
        "all_files = glob.glob(path + \"/*.txt\")\n",
        "\n",
        "glue = []\n",
        "\n",
        "for filename in all_files:\n",
        "  df = pd.read_csv(filename,\n",
        "                   names = [\"TimeStamp\", \"StationId\", \"StationTotalFlow\", \"StationAvgOcc\",\n",
        "                            \"StationAvgSpeed\", \"StationPercentObserved\", \"Lane1TotalFlow\",\n",
        "                            \"Lane1AvgOcc\", \"Lane1AvgSpeed\", \"Lane1Observed\", \"Lane2TotalFlow\",\n",
        "                            \"Lane2AvgOcc\", \"Lane2AvgSpeed\", \"Lane2Observed\", \"Lane3TotalFlow\",\n",
        "                            \"Lane3AvgOcc\", \"Lane3AvgSpeed\", \"Lane3Observed\", \"Lane4TotalFlow\",\n",
        "                            \"Lane4AvgOcc\", \"Lane4AvgSpeed\", \"Lane4Observed\"],\n",
        "                   usecols = np.arange(22))\n",
        "  glue.append(df)\n",
        "\n",
        "Data_Frame_2008 = pd.concat(glue, axis = 0, ignore_index = True) #2008 Loaded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jsUL_lRmNMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = r\"/content/drive/My Drive/PeMS_Data_Folder/2009\"\n",
        "all_files = glob.glob(path + \"/*.txt\")\n",
        "\n",
        "glue = []\n",
        "\n",
        "for filename in all_files:\n",
        "  df = pd.read_csv(filename,\n",
        "                   names = [\"TimeStamp\", \"StationId\", \"StationTotalFlow\", \"StationAvgOcc\",\n",
        "                            \"StationAvgSpeed\", \"StationPercentObserved\", \"Lane1TotalFlow\",\n",
        "                            \"Lane1AvgOcc\", \"Lane1AvgSpeed\", \"Lane1Observed\", \"Lane2TotalFlow\",\n",
        "                            \"Lane2AvgOcc\", \"Lane2AvgSpeed\", \"Lane2Observed\", \"Lane3TotalFlow\",\n",
        "                            \"Lane3AvgOcc\", \"Lane3AvgSpeed\", \"Lane3Observed\", \"Lane4TotalFlow\",\n",
        "                            \"Lane4AvgOcc\", \"Lane4AvgSpeed\", \"Lane4Observed\"],\n",
        "                   usecols = np.arange(22))\n",
        "  glue.append(df)\n",
        "\n",
        "Data_Frame_2009 = pd.concat(glue, axis = 0, ignore_index = True) #2009 Loaded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4Se64WboNZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = r\"/content/drive/My Drive/PeMS_Data_Folder/2010\"\n",
        "all_files = glob.glob(path + \"/*.txt\")\n",
        "\n",
        "glue = []\n",
        "\n",
        "for filename in all_files:\n",
        "  df = pd.read_csv(filename,\n",
        "                   names = [\"TimeStamp\", \"StationId\", \"StationTotalFlow\", \"StationAvgOcc\",\n",
        "                            \"StationAvgSpeed\", \"StationPercentObserved\", \"Lane1TotalFlow\",\n",
        "                            \"Lane1AvgOcc\", \"Lane1AvgSpeed\", \"Lane1Observed\", \"Lane2TotalFlow\",\n",
        "                            \"Lane2AvgOcc\", \"Lane2AvgSpeed\", \"Lane2Observed\", \"Lane3TotalFlow\",\n",
        "                            \"Lane3AvgOcc\", \"Lane3AvgSpeed\", \"Lane3Observed\", \"Lane4TotalFlow\",\n",
        "                            \"Lane4AvgOcc\", \"Lane4AvgSpeed\", \"Lane4Observed\"],\n",
        "                   usecols = np.arange(22))\n",
        "  glue.append(df)\n",
        "\n",
        "Data_Frame_2010 = pd.concat(glue, axis = 0, ignore_index = True) #2010 Loaded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7wIyFtkogBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = r\"/content/drive/My Drive/PeMS_Data_Folder/2011\"\n",
        "all_files = glob.glob(path + \"/*.txt\")\n",
        "\n",
        "glue = []\n",
        "\n",
        "for filename in all_files:\n",
        "  df = pd.read_csv(filename,\n",
        "                   names = [\"TimeStamp\", \"StationId\", \"StationTotalFlow\", \"StationAvgOcc\",\n",
        "                            \"StationAvgSpeed\", \"StationPercentObserved\", \"Lane1TotalFlow\",\n",
        "                            \"Lane1AvgOcc\", \"Lane1AvgSpeed\", \"Lane1Observed\", \"Lane2TotalFlow\",\n",
        "                            \"Lane2AvgOcc\", \"Lane2AvgSpeed\", \"Lane2Observed\", \"Lane3TotalFlow\",\n",
        "                            \"Lane3AvgOcc\", \"Lane3AvgSpeed\", \"Lane3Observed\", \"Lane4TotalFlow\",\n",
        "                            \"Lane4AvgOcc\", \"Lane4AvgSpeed\", \"Lane4Observed\"],\n",
        "                   usecols = np.arange(22))\n",
        "  glue.append(df)\n",
        "\n",
        "Data_Frame_2011 = pd.concat(glue, axis = 0, ignore_index = True) #2011 Loaded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCbpvreworMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = r\"/content/drive/My Drive/PeMS_Data_Folder/2012\"\n",
        "all_files = glob.glob(path + \"/*.txt\")\n",
        "\n",
        "glue = []\n",
        "\n",
        "for filename in all_files:\n",
        "  df = pd.read_csv(filename,\n",
        "                   names = [\"TimeStamp\", \"StationId\", \"StationTotalFlow\", \"StationAvgOcc\",\n",
        "                            \"StationAvgSpeed\", \"StationPercentObserved\", \"Lane1TotalFlow\",\n",
        "                            \"Lane1AvgOcc\", \"Lane1AvgSpeed\", \"Lane1Observed\", \"Lane2TotalFlow\",\n",
        "                            \"Lane2AvgOcc\", \"Lane2AvgSpeed\", \"Lane2Observed\", \"Lane3TotalFlow\",\n",
        "                            \"Lane3AvgOcc\", \"Lane3AvgSpeed\", \"Lane3Observed\", \"Lane4TotalFlow\",\n",
        "                            \"Lane4AvgOcc\", \"Lane4AvgSpeed\", \"Lane4Observed\"],\n",
        "                   usecols = np.arange(22))\n",
        "  glue.append(df)\n",
        "\n",
        "Data_Frame_2012 = pd.concat(glue, axis = 0, ignore_index = True) #2012 Loaded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgvH_Grzr5IM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.concat([Data_Frame_2008, Data_Frame_2009,\n",
        "                    Data_Frame_2010, Data_Frame_2011,\n",
        "                    Data_Frame_2012, Data_Frame_2013]) #Bind DataFrames by row in increasing order by year"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4_JTWjgswy4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max and c_prec == np.finfo(np.float16).precision:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df\n",
        "\n",
        "#Standard Memory Reducing Function.  Originally found this on kaggle about 2 years ago and have modified from time to time."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA09DOcLvtRG",
        "colab_type": "text"
      },
      "source": [
        "We have created a master dataset.  There is only 22 columns but 3 million observations for 22 columns is a large task to take on. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02I2ZcU9v-42",
        "colab_type": "text"
      },
      "source": [
        "First off we shall deal with the missing data.\n",
        "Lane2AvgOcc and Lane2AvgSpeed are objects but they should be float values.\n",
        "\n",
        "This has something to do with 2014 data. Will remove for now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37rZNMUH9g6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = reduce_mem_usage(df) #Reduced Memory by 30%!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onXqaokomsCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Lane4TotalFlow'].fillna((df['Lane4TotalFlow'].mean()), inplace=True)\n",
        "df['Lane4AvgOcc'].fillna((df['Lane4AvgOcc'].mean()), inplace=True)\n",
        "df['Lane4AvgSpeed'].fillna((df['Lane4AvgSpeed'].mean()), inplace=True)\n",
        "df['Lane4Observed'].fillna((df['Lane4Observed'].mean()), inplace=True) #Fill in Missing values with there respective means"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4xSSTlYtwD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[\"NewTimeStamp\"] = pd.to_datetime(df[\"TimeStamp\"]) #Create a TimeStamp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZJZPBmwXvJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['year'] = df['NewTimeStamp'].dt.year\n",
        "df['month'] = df['NewTimeStamp'].dt.month\n",
        "df['day'] = df['NewTimeStamp'].dt.day\n",
        "df['hour'] = df['NewTimeStamp'].dt.hour\n",
        "df['minute'] = df['NewTimeStamp'].dt.minute\n",
        "df['dayofweek'] = df[\"NewTimeStamp\"].dt.dayofweek #Extract Information from Time Stamp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyxIXJzF1vJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Sunday(dayofweek):\n",
        "  if dayofweek == 1:\n",
        "    return 3\n",
        "  elif dayofweek == 2:\n",
        "    return 4\n",
        "  elif dayofweek == 3:\n",
        "    return 5\n",
        "  elif dayofweek == 4:\n",
        "    return 6\n",
        "  elif dayofweek == 5:\n",
        "    return 7\n",
        "  elif dayofweek == 6:\n",
        "    return 1\n",
        "  elif dayofweek == 0:\n",
        "    return 2\n",
        "  else:\n",
        "    return dayofweek\n",
        "  \n",
        "df[\"dayofweek\"] = df[\"dayofweek\"].apply(Sunday)\n",
        "df[\"dayofweek\"].value_counts() # Convert day of the week from 0-6 to 1-7......(1 -> Sunday)......(7 -> Saturday)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M4F8k-JaqFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_wide = df.pivot_table(index = [\"StationId\", \"day\", \"month\", \"year\", \"hour\", \"dayofweek\"], columns = \"minute\", values = [\"StationTotalFlow\"])\n",
        "#Create a Multi Index Dataframe\n",
        "#This will help us parse the data how we want it. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcKpGAxHbfUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_wide.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDYQ803H8U1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_wide = df_wide.reset_index() # Flatten Dimensions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUcG71xO8dOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_wide.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPRwgME2iPq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_wide.columns = df_wide.columns.get_level_values(0) # Flatten Dimensions again\n",
        "df_wide.head()  # We are now back into are original format"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj6XKdwht1zc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def df_column_uniquify(df_wide):\n",
        "    df_columns = df_wide.columns\n",
        "    new_columns = []\n",
        "    for item in df_columns:\n",
        "        counter = 0\n",
        "        newitem = item\n",
        "        while newitem in new_columns:\n",
        "            counter += 1\n",
        "            newitem = \"{}_{}\".format(item, counter)\n",
        "        new_columns.append(newitem)\n",
        "    df_wide.columns = new_columns\n",
        "    return df_wide\n",
        "  \n",
        "df_wide = df_column_uniquify(df_wide)\n",
        "\n",
        "df_wide.rename(columns={'StationTotalFlow': 'flow_00',\n",
        "                        'StationTotalFlow_1': 'flow_05',\n",
        "                        'StationTotalFlow_2': 'flow_10',\n",
        "                        'StationTotalFlow_3': 'flow_15',\n",
        "                        'StationTotalFlow_4': 'flow_20',\n",
        "                        'StationTotalFlow_5': 'flow_25',\n",
        "                        'StationTotalFlow_6': 'flow_30',\n",
        "                        'StationTotalFlow_7': 'flow_35',\n",
        "                        'StationTotalFlow_8': 'flow_40',\n",
        "                        'StationTotalFlow_9': 'flow_45',\n",
        "                        'StationTotalFlow_10': 'flow_50',\n",
        "                        'StationTotalFlow_11': 'flow_55'}, inplace=True)  #Change the Stationflows names to match their actual 5 min intervals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raduqh4ilyN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_wide.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlWC9WyGzoKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_wide['flow_00'].fillna((df_wide['flow_00'].mean()), inplace=True) #Fill Na with mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq2iCTS3GuRJ",
        "colab_type": "text"
      },
      "source": [
        "#We have successful parsed the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2X06dkbK6Gh",
        "colab_type": "text"
      },
      "source": [
        "We normalized and divided the dataframe into training and testing to be fed to our deep neural network model.\n",
        "\n",
        "The CNN model will learn a function that maps a sequence of past observations as input to an output observation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxd8oTUNK8It",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "X, y = df_wide.loc[:, df_wide.columns != \"flow_55\"], df_wide[\"flow_55\"]\n",
        "\n",
        "transformer = Normalizer().fit(X)\n",
        "X = transformer.transform(X)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 7984427) # Create Training and Validation Sets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu-pJQok6yiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.expand_dims(X_train, axis=2)\n",
        "X_test = np.expand_dims(X_test, axis=2) # Expand Dimensions for Convolutional Network"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih7bCs1LXv7e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = y_train.values.reshape(-1, 1)\n",
        "y_test = y_test.values.reshape(-1, 1) # Make an Array\n",
        "\n",
        "#y_train = sc.fit_transform(y_train)\n",
        "#y_test = sc.transform(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEcnv0_DOqIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Best Model so far\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv1D, Dropout, MaxPooling1D, Flatten\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "print('Build model...')\n",
        "\n",
        "modelx = Sequential()\n",
        "modelx.add(Conv1D(filters = 64, kernel_size=3, activation='relu', input_shape=(17,1)))\n",
        "modelx.add(Conv1D(filters = 32, kernel_size=3, activation='relu'))\n",
        "modelx.add(Dropout(0.2))\n",
        "modelx.add(MaxPooling1D(pool_size=2))\n",
        "modelx.add(Flatten())\n",
        "modelx.add(Dense(50, activation = \"relu\"))\n",
        "modelx.add(Dense(1))\n",
        "\n",
        "modelx.compile(loss='mean_squared_error', optimizer = 'adam')\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=50, verbose=1, mode='auto', restore_best_weights=True)\n",
        "print('Train...')\n",
        "\n",
        "historyx = modelx.fit(X_train, y_train, batch_size=100, validation_data=(X_test, y_test),\n",
        "                    callbacks=[monitor], verbose=2, epochs=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crOTXSInJ7Bw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot an estimation of our models performance\n",
        "\n",
        "plt.plot(historyx.history['loss'])\n",
        "plt.plot(historyx.history['val_loss'])\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel(\"Mean Square Error\")\n",
        "plt.title(\"Loss & Cost over Time\")\n",
        "plt.legend([\"Training\", \"Validating\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUpnG65dKUJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "pred = modelx.predict(X_test)\n",
        "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
        "print(\"Score (RMSE): {}\".format(score)) # Rmse to compare with the standard deviation of our target variable(flow_55) which is 150"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WkyFnjAfYgo",
        "colab_type": "text"
      },
      "source": [
        "We have shown a cnn with are training data normalized.  This is a technique used in practice for temporial data+cnn but it is not desirable.  Using this method we are saying that the day of the month aka 7 is less than the day of the month 8... this is not true.  This same concept can be thought for \n",
        "\n",
        "StationId, day, month, year, hour, and dayofweek"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkRSR2mwgByh",
        "colab_type": "text"
      },
      "source": [
        "Are more ideal way to represent time patterns is to apply fourier transforms to our data or one-hot encoding.  There are better algorithms to use than cnn for these methods.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_zcZ0AFh24n",
        "colab_type": "text"
      },
      "source": [
        "That's a lot a values to encode...\n",
        "What's a good way to go about this..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GEKu_exWwVX",
        "colab_type": "text"
      },
      "source": [
        "**PART 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oYrFSLThlKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_wide_V2 = df_wide\n",
        "df_wide_V2['Date'] = pd.to_datetime(df_wide_V2[['year','month','day']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPXD-O0o_RO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_wide_V2['dayofweek'] = pd.Categorical(df_wide_V2.dayofweek)\n",
        "df_wide_V2['month'] = pd.Categorical(df_wide_V2.month)\n",
        "df_wide_V2['StationId'] = pd.Categorical(df_wide_V2.StationId)\n",
        "df_wide_V2['day'] = pd.Categorical(df_wide_V2.day)\n",
        "df_wide_V2['year'] = pd.Categorical(df_wide_V2.year)\n",
        "df_wide_V2['hour'] = pd.Categorical(df_wide_V2.hour)\n",
        "\n",
        "df_wide_V2 = df_wide_V2.set_index('Date')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkiz7i3zU82l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target = [\"flow_55\"]\n",
        "\n",
        "df_cat = [\"StationId\", \"day\", \"month\", \"year\", \"hour\", \"dayofweek\"]\n",
        "\n",
        "df_con = [\"flow_00\", \"flow_05\", \"flow_10\", \"flow_15\", \"flow_20\",\n",
        "          \"flow_25\", \"flow_30\", \"flow_35\", \"flow_40\", \"flow_45\", \"flow_50\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrexApkFV5Kf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "train = df_wide_V2[df_wide_V2.index < datetime.datetime(2012, 11, 1)]  \n",
        "test = df_wide_V2[df_wide_V2.index >= datetime.datetime(2012, 11, 1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym0JU2RxC02n",
        "colab_type": "text"
      },
      "source": [
        "After doing some research... we have decided to see how well deep encoding will help us in our exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct_Dn84QnxOz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = train[df_cat + df_con].copy()  \n",
        "X_test = test[df_cat + df_con].copy()  \n",
        "y = train[target].copy()  \n",
        "y_test = test[target].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chZQYi14oV3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "scaler = Normalizer()  \n",
        "X.loc[:, df_con] = scaler.fit_transform(X[df_con].values)  \n",
        "X_test.loc[:, df_con] = scaler.transform(X_test[df_con].values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4htjgeepSo8",
        "colab_type": "code",
        "outputId": "51a4e117-fc89-40f4-8978-89d1042b24de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "X.shape, X_test.shape, y.shape, y_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1426679, 17), (321519, 17), (1426679, 1), (321519, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX7wni_Dplde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat_sizes = [(c, len(X[c].cat.categories)) for c in df_cat]  \n",
        "cat_sizes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po3F11aip4Bc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_sizes = [(c, min(50, (c + 1) // 2)) for _, c in cat_sizes]  \n",
        "embedding_sizes"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}